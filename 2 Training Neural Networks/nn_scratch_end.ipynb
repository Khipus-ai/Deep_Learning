{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3908a6",
   "metadata": {},
   "source": [
    "# Khipus.ai\n",
    "## Neural Network from Scratch\n",
    "### Case Study: \n",
    "<span>Â© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>\n",
    "---\n",
    "This notebook implements a simple neural network from scratch using NumPy. We will build, train, and evaluate a basic single-layer perceptron on a heart disease dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6840c",
   "metadata": {},
   "source": [
    "## 1. Importing Required Packages\n",
    "We begin by importing necessary Python libraries for data processing, visualization, and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62491994",
   "metadata": {},
   "source": [
    "## 2. Loading and Preprocessing the Dataset\n",
    "We use the heart disease dataset to train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e93bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset (Assume 'heart.csv' is available)\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d633085c",
   "metadata": {},
   "source": [
    "## 3. Selecting Features and Labels\n",
    "We separate the independent variables (features) from the dependent variable (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and labels\n",
    "X = np.array(df.loc[:, df.columns != 'output'])\n",
    "y = np.array(df['output'])\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"X: {X.shape}, y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b409e15",
   "metadata": {},
   "source": [
    "## 4. Splitting Data into Training and Testing Sets\n",
    "We split the dataset into 80% training and 20% testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b4f0f",
   "metadata": {},
   "source": [
    "## 5. Scaling Features\n",
    "To improve training performance, we scale the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267468e",
   "metadata": {},
   "source": [
    "## 6. Implementing a Neural Network from Scratch\n",
    "We define a single-layer perceptron with a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230376f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetworkFromScratch:\n",
    "    def __init__(self, LR, X_train, y_train, X_test, y_test):\n",
    "        self.w = np.random.randn(X_train.shape[1])\n",
    "        self.b = np.random.randn()\n",
    "        self.LR = LR\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.L_train = []\n",
    "        self.L_test = []\n",
    "        \n",
    "    def activation(self, x):\n",
    "        # Sigmoid function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def dactivation(self, x):\n",
    "        # Derivative of sigmoid function\n",
    "        return self.activation(x) * (1 - self.activation(x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_1 = np.dot(X, self.w) + self.b\n",
    "        activate_1 = self.activation(hidden_1)\n",
    "        return activate_1\n",
    "\n",
    "    def backward(self, X, y_true):\n",
    "        # Compute gradients\n",
    "        hidden_1 = np.dot(X, self.w) + self.b\n",
    "        y_pred = self.forward(X)\n",
    "        dL_dpred = 2 * (y_pred - y_true)\n",
    "        dpred_dhidden1 = self.dactivation(hidden_1)\n",
    "        dhidden1_db = 1\n",
    "        dhidden1_dw = X\n",
    "\n",
    "        dL_db = dL_dpred * dpred_dhidden1 * dhidden1_db\n",
    "        dL_dw = dL_dpred * dpred_dhidden1 * dhidden1_dw\n",
    "        return dL_db, dL_dw\n",
    "\n",
    "    def optimizer(self, dL_db, dL_dw):\n",
    "        # Update weights\n",
    "        self.b = self.b - dL_db * self.LR\n",
    "        self.w = self.w - dL_dw * self.LR\n",
    "\n",
    "    def train(self, ITERATIONS):\n",
    "        for i in range(ITERATIONS):\n",
    "            random_pos = np.random.randint(len(self.X_train))\n",
    "            y_train_true = self.y_train[random_pos]\n",
    "            y_train_pred = self.forward(self.X_train[random_pos])\n",
    "            \n",
    "            # Compute training loss\n",
    "            L = np.sum(np.square(y_train_pred - y_train_true))\n",
    "            self.L_train.append(L)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dL_db, dL_dw = self.backward(\n",
    "                self.X_train[random_pos], self.y_train[random_pos]\n",
    "            )\n",
    "            \n",
    "            # Update weights\n",
    "            self.optimizer(dL_db, dL_dw)\n",
    "\n",
    "            # Compute test error\n",
    "            L_sum = 0\n",
    "            for j in range(len(self.X_test)):\n",
    "                y_true = self.y_test[j]\n",
    "                y_pred = self.forward(self.X_test[j])\n",
    "                L_sum += np.square(y_pred - y_true)\n",
    "            self.L_test.append(L_sum)\n",
    "\n",
    "        return \"Training successfully finished\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66dc066",
   "metadata": {},
   "source": [
    "## 7. Training the Model\n",
    "We train the neural network using a simple gradient descent approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1011c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "LR = 0.1\n",
    "ITERATIONS = 1000\n",
    "\n",
    "# Create and train the model\n",
    "nn = NeuralNetworkFromScratch(LR=LR, X_train=X_train_scale, y_train=y_train, X_test=X_test_scale, y_test=y_test)\n",
    "nn.train(ITERATIONS=ITERATIONS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469d9e0",
   "metadata": {},
   "source": [
    "## 8. Visualizing Training Loss\n",
    "We plot the loss over training iterations to observe convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot loss\n",
    "sns.lineplot(x=list(range(len(nn.L_test))), y=nn.L_test)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Training Iterations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4674ce0",
   "metadata": {},
   "source": [
    "## 9. Evaluating the Model\n",
    "We test the model's accuracy by making predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec365afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model evaluation\n",
    "total = X_test_scale.shape[0]\n",
    "correct = 0\n",
    "y_preds = []\n",
    "\n",
    "for i in range(total):\n",
    "    y_true = y_test[i]\n",
    "    y_pred = np.round(nn.forward(X_test_scale[i]))\n",
    "    y_preds.append(y_pred)\n",
    "    correct += 1 if y_true == y_pred else 0\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6e107",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix\n",
    "We compute a confusion matrix to analyze classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656dd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
