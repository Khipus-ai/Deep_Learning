{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khipus.ai\n",
    "## Convolutional Neural Network with PyTorch\n",
    "### Case Study: Multi-Class image classification -Handwritten digit classification\n",
    "<span>Â© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>\n",
    "\n",
    "This notebook demonstrates how to build a convolutional neural network (CNN) using PyTorch to classify images from the MNIST dataset. We will cover basic tensor operations, the structure of a CNN, and the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset Overview\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology. It is a widely used dataset in machine learning, particularly for training and testing image classification models.The MNIST dataset is a collection of 70,000 handwritten digits, commonly used for training various image processing systems. It is a standard benchmark in the field of machine learning and computer vision.\n",
    "\n",
    "#### Dataset Structure\n",
    "\n",
    "The MNIST dataset consists of:\n",
    "\n",
    "- **Training set**: 60,000 images\n",
    "- **Test set**: 10,000 images\n",
    "\n",
    "Each image is a grayscale image of size 28x28 pixels, representing a single digit from 0 to 9.\n",
    "\n",
    "This example is using MNIST handwritten digits.The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 255. \n",
    "\n",
    "In this example, each image will be converted to float32 and normalized to [0, 1].\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Import the Libraries \n",
    "2. Loading the MNIST Dataset\n",
    "3. Building the Convolutional Neural Network\n",
    "4. Training the Model\n",
    "5. Evaluating the Model\n",
    "6. Visualizing the model predictions\n",
    "\n",
    "## 1. Import the Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanikj\\AppData\\Local\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the torch module for tensor operations and deep learning\n",
    "import torch\n",
    "# Import numpy library for numerical computing\n",
    "import numpy as np\n",
    "# Import matplotlib.pyplot for plotting graphs and images\n",
    "import matplotlib.pyplot as plt \n",
    "# Import the transforms module from torchvision to perform image transformations (e.g., converting to tensors, normalizing)\n",
    "import torchvision.transforms as transforms\n",
    "# Import the datasets module from torchvision to access pre-built datasets like MNIST\n",
    "from torchvision import datasets\n",
    "# Import DataLoader from torch.utils.data to conveniently load data in batches\n",
    "from torch.utils.data import DataLoader\n",
    "# Import the torch.nn module as nn, which provides classes for building neural networks\n",
    "import torch.nn as nn  \n",
    "# Import the torch.nn.functional module as F, which contains functions for various operations like activations and pooling\n",
    "import torch.nn.functional as F \n",
    "# Import optimization algorithms module from PyTorch as 'optim'\n",
    "import torch.optim as optim  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the MNIST Dataset\n",
    "\n",
    "We will use the `torchvision` library to load the MNIST dataset. This dataset consists of 28x28 pixel images of handwritten digits (0-9).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a composition of image transformations using transforms.Compose:\n",
    "# 1. Convert the PIL image to a PyTorch tensor.\n",
    "# 2. Normalize the tensor image with mean and std of 0.5 for the single channel image.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor format\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize image tensor with mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# Load the MNIST training dataset:\n",
    "# - root: path where MNIST data is stored or will be downloaded.\n",
    "# - train=True: specifies that we want the training split.\n",
    "# - download=True: downloads the dataset if it's not already available in the given root.\n",
    "# - transform: applies the defined transformations.\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the MNIST test dataset with similar options, except train=False to choose the test split.\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create a DataLoader for the training dataset:\n",
    "# - dataset: pass the train_dataset.\n",
    "# - batch_size: set to 64 samples per batch.\n",
    "# - shuffle: set to True to randomly shuffle data at every epoch.\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the test dataset:\n",
    "# - dataset: pass the test_dataset.\n",
    "# - batch_size: set to 64 for batching, ensuring consistency with training.\n",
    "# - shuffle: set to False to preserve the order of test samples.\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Convolutional Neural Network\n",
    "\n",
    "Now, let's define our CNN architecture. We will create a simple model with two convolutional layers followed by fully connected layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a Convolutional Neural Network (CNN) class that inherits from nn.Module\n",
    "class CNN(nn.Module):\n",
    "    # Constructor method for initializing the network layers\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class (nn.Module)\n",
    "        super(CNN, self).__init__()\n",
    "        # Define the first convolutional layer:\n",
    "        # 1 input channel (grayscale image), 32 output channels (filters), 3x3 kernel, stride of 1, and padding of 1 to maintain spatial dimensions\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Define the second convolutional layer:\n",
    "        # 32 input channels, 64 output channels, 3x3 kernel, stride of 1, and padding of 1\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # Define the first fully connected (linear) layer:\n",
    "        # The input feature dimension is 64*7*7 (after two pooling layers reduce the spatial dimensions), and outputs 128 features\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        # Define the second fully connected layer:\n",
    "        # Takes 128 features and outputs 10 classes (for the digits 0-9 in MNIST)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    # Define the forward pass method which specifies how input data passes through the model\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolutional layer to input x and then apply ReLU activation function\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Apply 2D max pooling with a kernel size of 2 to downsample the feature map after the first conv layer\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Apply the second convolutional layer to the pooled features followed by ReLU activation function\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Apply 2D max pooling with a kernel size of 2 to further reduce the spatial dimensions\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Reshape the tensor x to have a shape appropriate for the fully connected layers\n",
    "        # '-1' lets PyTorch automatically determine the batch size; the rest is flattened feature size 64 * 7 * 7\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        # Pass the flattened tensor through the first fully connected layer and apply ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pass the output through the second fully connected layer to get raw scores for each class\n",
    "        x = self.fc2(x)\n",
    "        # Return the final output scores (logits)\n",
    "        return x\n",
    "\n",
    "# Instantiate the CNN model by creating an instance of the CNN class\n",
    "model = CNN()\n",
    "# Print the model's architecture to inspect its layers and parameters\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "\n",
    "Next, we will define the loss function and the optimizer, and then train the model on the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0132\n",
      "Epoch [2/5], Loss: 0.0374\n",
      "Epoch [3/5], Loss: 0.0015\n",
      "Epoch [4/5], Loss: 0.1126\n",
      "Epoch [5/5], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Instantiate the CrossEntropyLoss function for multi-class classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Create the Adam optimizer to update model parameters with a learning rate of 0.001\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Set the total number of epochs (complete passes over the dataset) to 5\n",
    "for epoch in range(num_epochs):  # Loop over each epoch of training\n",
    "    for images, labels in train_loader:  # Iterate over batches of images and their corresponding labels from the training data loader\n",
    "        optimizer.zero_grad()  # Zero the gradients for all model parameters before backpropagation (prevents gradient accumulation)\n",
    "        outputs = model(images)  # Forward pass: Compute the model's output predictions for the given batch of images\n",
    "        loss = criterion(outputs, labels)  # Compute the loss between the predicted outputs and the true labels using the defined criterion\n",
    "        loss.backward()  # Backward pass: Compute the gradients of the loss with respect to the model parameters\n",
    "        optimizer.step()  # Update the model's parameters using the computed gradients (this adjusts the weights)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')  # Print the epoch number and the final batch loss for monitoring training progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating the Model\n",
    "\n",
    "Finally, we will evaluate the model's performance on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 99.10%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the counter for the number of correct predictions to zero\n",
    "correct = 0  \n",
    "# Initialize the counter for the total number of samples to zero\n",
    "total = 0  \n",
    "\n",
    "# Disable gradient computations since we're in inference mode (evaluating the model)\n",
    "with torch.no_grad():\n",
    "    # Loop through the test_loader to access batches of test images and their corresponding labels\n",
    "    for images, labels in test_loader:\n",
    "        # Obtain the model's output predictions for the current batch of images\n",
    "        outputs = model(images)\n",
    "        # Find the index (class) with the highest score for each image and ignore the scores\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Increment the total counter by the number of labels in the current batch\n",
    "        total += labels.size(0)\n",
    "        # Compare predicted labels with actual labels and sum the number of correct predictions,\n",
    "        # then increment the correct counter by that sum (converted to a Python number)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate the accuracy as a percentage of correct predictions over total samples\n",
    "accuracy = 100 * correct / total\n",
    "# Print the resulting accuracy with two decimal places of precision\n",
    "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Get one batch from test_loader as a validation set\n",
    "dataiter = iter(test_loader)  # Create an iterator from test_loader to iterate over batches of data\n",
    "val_images, val_labels = next(dataiter)  # Retrieve the next batch of images and the corresponding labels from the iterator\n",
    "\n",
    "indices = np.random.choice(val_images.shape[0], 5, replace=False)  # Randomly select 5 unique indices from the batch (without replacement)\n",
    "val_images5 = val_images[indices]  # Use the selected indices to extract 5 images from the validation images\n",
    "\n",
    "# Get the model's predictions\n",
    "model.eval()  # Set the model to evaluation mode, affecting layers like dropout or batch normalization\n",
    "with torch.no_grad():  # Disable gradient computation since we are only doing inference (saves memory and computations)\n",
    "    outputs = model(val_images5)  # Pass the selected images through the model to obtain output logits\n",
    "    _, predictions = torch.max(outputs, 1)  # Compute the index of the maximum logit per image, which corresponds to the predicted class\n",
    "\n",
    "# Plot the images with their predicted labels\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))  # Create a figure and a 1x5 grid of subplots with specified figure size\n",
    "for i, ax in enumerate(axes):  # Iterate over each subplot axis and its corresponding index\n",
    "    ax.imshow(val_images5[i].squeeze(), cmap='gray')  # Display the i-th image after squeezing extra dimensions; use grayscale colormap\n",
    "    ax.set_title(f'Pred: {predictions[i].item()}')  # Set the title for this subplot to display the predicted label (convert tensor to value)\n",
    "    ax.axis('off')  # Remove the axis ticks and labels for a cleaner image display\n",
    "plt.show()  # Render and display the plot with the images and their predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
